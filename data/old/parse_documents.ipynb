{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotebookLoader\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/robertgshaw/dev/llms/langchain-gpt/memory/data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_cell_str = \"'markdown' cell: \"\n",
    "md_cell_str_len = len(md_cell_str)\n",
    "code_cell_str = \"'code' cell: \"\n",
    "code_cell_str_len = len(code_cell_str)\n",
    "\n",
    "def find_next_index(str):\n",
    "    md_ind = str.find(md_cell_str)\n",
    "    code_ind = str.find(code_cell_str)\n",
    "\n",
    "    if md_ind == -1:\n",
    "        return code_ind\n",
    "    elif code_ind == -1:\n",
    "        return md_ind\n",
    "    else:\n",
    "        return min(md_ind, code_ind)\n",
    "\n",
    "def extract_text(document):\n",
    "    notebook_txt = document.page_content\n",
    "    saved_txts = []\n",
    "    saved_txt = \"\"\n",
    "\n",
    "    first_code_cell = True\n",
    "    is_code_cell = False\n",
    "    is_first_cell = True\n",
    "\n",
    "    while True:\n",
    "        if notebook_txt[:md_cell_str_len] == md_cell_str:\n",
    "            notebook_txt = notebook_txt[md_cell_str_len:]\n",
    "            first_code_cell = True\n",
    "            is_code_cell = False\n",
    "\n",
    "        elif notebook_txt[:code_cell_str_len] == code_cell_str:\n",
    "            notebook_txt = notebook_txt[code_cell_str_len:]\n",
    "            is_code_cell = True\n",
    "            if first_code_cell:\n",
    "                saved_txt += \"Here is some code:\\n\"\n",
    "                first_code_cell = False\n",
    "            \n",
    "        else:\n",
    "            notebook_txt = notebook_txt[line_end:].rstrip()\n",
    "            assert len(notebook_txt) == 0\n",
    "            saved_txts.append(saved_txt)\n",
    "            break\n",
    "\n",
    "        line_end = notebook_txt.find(\"\\n\")\n",
    "        line_lst = ast.literal_eval(notebook_txt[1:line_end-1])\n",
    "        \n",
    "        if not is_first_cell and not is_code_cell and len(line_lst) > 0 and len(line_lst[0]) >= 2 and line_lst[0][:2] == \"##\":\n",
    "            saved_txts.append(saved_txt)\n",
    "            saved_txt = \"\"\n",
    "\n",
    "        for str in line_lst:\n",
    "            saved_txt += str\n",
    "            saved_txt += (\"\\n\" if is_code_cell else \" \")\n",
    "        saved_txt += \"\\n\"\n",
    "        \n",
    "        next_ind = find_next_index(notebook_txt)\n",
    "        if next_ind != -1:\n",
    "            notebook_txt = notebook_txt[next_ind:]\n",
    "\n",
    "        is_first_cell = False\n",
    "        \n",
    "    return saved_txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Plan and Execute  Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks. This idea is largely inspired by [BabyAGI](https://github.com/yoheinakajima/babyagi) and then the [\"Plan-and-Solve\" paper](https://arxiv.org/abs/2305.04091).  The planning is almost always done by an LLM.  The execution is usually done by a separate agent (equipped with tools). \n",
      "\n",
      "## Imports \n",
      "Here is some code:\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
      "from langchain.llms import OpenAI\n",
      "from langchain import SerpAPIWrapper\n",
      "from langchain.agents.tools import Tool\n",
      "from langchain import LLMMathChain\n",
      "\n",
      "\n",
      "## Tools \n",
      "Here is some code:\n",
      "search = SerpAPIWrapper()\n",
      "llm = OpenAI(temperature=0)\n",
      "llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
      "tools = [\n",
      "    Tool(\n",
      "        name = \"Search\",\n",
      "        func=search.run,\n",
      "        description=\"useful for when you need to answer questions about current events\"\n",
      "    ),\n",
      "    Tool(\n",
      "        name=\"Calculator\",\n",
      "        func=llm_math_chain.run,\n",
      "        description=\"useful for when you need to answer questions about math\"\n",
      "    ),\n",
      "]\n",
      "\n",
      "\n",
      "## Planner, Executor, and Agent \n",
      "Here is some code:\n",
      "model = ChatOpenAI(temperature=0)\n",
      "\n",
      "planner = load_chat_planner(model)\n",
      "\n",
      "executor = load_agent_executor(model, tools, verbose=True)\n",
      "\n",
      "agent = PlanAndExecute(planner=planner, executer=executor, verbose=True)\n",
      "\n",
      "\n",
      "## Run Example \n",
      "Here is some code:\n",
      "agent.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\")\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loader = NotebookLoader(\n",
    "        # './langchain/docs/modules/memory/getting_started.ipynb',\n",
    "        './langchain/docs/modules/agents/plan_and_execute.ipynb',\n",
    "        include_outputs=False, \n",
    "        max_output_length=2000, \n",
    "        remove_newline=True)\n",
    "\n",
    "document = loader.load()[0]\n",
    "for text in extract_text(document):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "\n",
    "base_dir = \"./langchain/docs/modules\"\n",
    "python_notebook_paths = []\n",
    "for (dirpath, dirnames, filenames) in walk(base_dir):\n",
    "    for filename in filenames:\n",
    "        _, ext = filename.split(\".\")\n",
    "        if ext == \"ipynb\":\n",
    "            python_notebook_paths.append(f\"{dirpath}/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "data = []\n",
    "idx = 0\n",
    "for path in python_notebook_paths:\n",
    "    loader = NotebookLoader(\n",
    "        path,\n",
    "        include_outputs=False, \n",
    "        max_output_length=2000, \n",
    "        remove_newline=True)\n",
    "    document = loader.load()[0]\n",
    "    text_chunks = (extract_text(document))\n",
    "\n",
    "    for text_chunk in text_chunks:\n",
    "        data.append ({\n",
    "            \"idx\": idx,\n",
    "            \"text\": text_chunk,\n",
    "            \"token_len\": len(encoding.encode(text_chunk)),\n",
    "            \"source\": document.metadata[\"source\"]\n",
    "        })\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_records(data)\n",
    "df.to_csv(\"langchain_docs_modules.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"langchain_docs_modules.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
