{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"langchain_docs_modules.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "documents = []\n",
    "for index, row in df.iterrows():    \n",
    "    documents.append(Document(page_content=row[\"text\"],metadata={\"source\": row[\"source\"]}))\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "db = Chroma.from_documents(documents, embeddings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"Use the following pieces of context from the LangChain documentation to write code based on the question below. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Respond with Python code including importing all required packages:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo') # switch to 'gpt-4'\n",
    "qa = RetrievalQA.from_chain_type(llm=model, chain_type=\"stuff\", retriever=db.as_retriever(), chain_type_kwargs=chain_type_kwargs, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"How do I write a custom prompt template in LangChain? Provide an example\" \n",
    "query = \"How do I use LLM chain?\" \n",
    "result = qa(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "from langchain import PromptTemplate, OpenAI, LLMChain\n",
      "\n",
      "# Define a prompt template\n",
      "prompt_template = \"What is a good name for a company that makes {product}?\"\n",
      "\n",
      "# Initialize the OpenAI LLM model\n",
      "llm = OpenAI(temperature=0)\n",
      "\n",
      "# Initialize the LLMChain with the prompt template and the OpenAI LLM model\n",
      "llm_chain = LLMChain(\n",
      "    llm=llm,\n",
      "    prompt=PromptTemplate.from_template(prompt_template)\n",
      ")\n",
      "\n",
      "# Use the LLMChain to generate a company name based on the input product\n",
      "llm_chain(\"colorful socks\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product': 'colorful socks', 'text': '\\n\\nSocktastic!'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = \"What is a good name for a company that makes {product}?\"\n",
    "\n",
    "# Initialize the OpenAI LLM model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Initialize the LLMChain with the prompt template and the OpenAI LLM model\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "\n",
    "# Use the LLMChain to generate a company name based on the input product\n",
    "llm_chain(\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How do I write a custom prompt template in LangChain? Provide an example',\n",
       " 'result': 'from langchain.prompts.prompt import PromptTemplate\\n\\n# Define the custom prompt template\\nprompt_template = \"\"\"Given the function name \\'{function_name}\\', write an English language explanation of the function.\\nFunction Definition:\\n{function_source}\\n\"\"\"\\n\\n# Define the input variables for the prompt template\\ninput_variables = [\"function_name\", \"function_source\"]\\n\\n# Create a PromptTemplate object with the custom prompt template and input variables\\ncustom_prompt = PromptTemplate(template=prompt_template, input_variables=input_variables)\\n\\n# Create an LLMChain object with the custom prompt template\\nllm = OpenAI(temperature=0)\\nllm_chain = LLMChain(prompt=custom_prompt, llm=llm, verbose=True)',\n",
       " 'source_documents': [Document(page_content=\"# How to create a custom prompt template  Let's suppose we want the LLM to generate English language explanations of a function given its name. To achieve this task, we will create a custom prompt template that takes in the function name as input, and formats the prompt template to provide the source code of the function.  ## Why are custom prompt templates needed?  LangChain provides a set of default prompt templates that can be used to generate prompts for a variety of tasks. However, there may be cases where the default prompt templates do not meet your needs. For example, you may want to create a prompt template with specific dynamic instructions for your language model. In such cases, you can create a custom prompt template.  Take a look at the current set of default prompt templates [here](../getting_started.md). \\n\", metadata={'source': 'langchain/docs/modules/prompts/prompt_templates/examples/custom_prompt_template.ipynb'}),\n",
       "  Document(page_content='## Customize Prompt You can also customize the prompt that is used. Here is an example prompting to avoid using the \\'echo\\' utility \\nHere is some code:\\nfrom langchain.prompts.prompt import PromptTemplate\\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\\n\\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\\nQuestion: \"copy the files in the directory named \\'target\\' into a new directory at the same level as target called \\'myNewDirectory\\'\"\\nI need to take the following actions:\\n- List all files in the directory\\n- Create a new directory\\n- Copy the files from the first directory into the second directory\\n```bash\\nls\\nmkdir myNewDirectory\\ncp -r target/* myNewDirectory\\n```\\n\\nDo not use \\'echo\\' when writing the script.\\n\\nThat is the format. Begin!\\nQuestion: {question}\"\"\"\\n\\nPROMPT = PromptTemplate(input_variables=[\"question\"], template=_PROMPT_TEMPLATE, output_parser=BashOutputParser())\\n\\nbash_chain = LLMBashChain.from_llm(llm, prompt=PROMPT, verbose=True)\\n\\ntext = \"Please write a bash script that prints \\'Hello World\\' to the console.\"\\n\\nbash_chain.run(text)\\n\\n', metadata={'source': 'langchain/docs/modules/chains/examples/llm_bash.ipynb'}),\n",
       "  Document(page_content='## Chaining \\nHere is some code:\\nfrom langchain import PromptTemplate, LLMChain\\n\\ntemplate = \"\"\"Question: {question}\\n\\nAnswer: Let\\'s think step by step.\"\"\"\\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\\nllm_chain = LLMChain(prompt=prompt, llm=pgllm, verbose=True)\\n\\nquestion = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\\n\\nllm_chain.predict(question=question)\\n\\ntemplate = \"\"\"Write a {adjective} poem about {subject}.\"\"\"\\nprompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"subject\"])\\nllm_chain = LLMChain(prompt=prompt, llm=pgllm, verbose=True)\\n\\nllm_chain.predict(adjective=\"sad\", subject=\"ducks\")\\n\\n\\n', metadata={'source': 'langchain/docs/modules/models/llms/integrations/predictionguard.ipynb'}),\n",
       "  Document(page_content='## Set Up LLM Chain with Custom Prompt  Next, let\\'s set up a simple LLM chain but give it a custom prompt for blog post generation. Note that the custom prompt is parameterized and takes two inputs: `context`, which will be the documents fetched from the vector search, and `topic`, which is given by the user. \\nHere is some code:\\nfrom langchain.chains import LLMChain\\nprompt_template = \"\"\"Use the context below to write a 400 word blog post about the topic below:\\n    Context: {context}\\n    Topic: {topic}\\n    Blog post:\"\"\"\\n\\nPROMPT = PromptTemplate(\\n    template=prompt_template, input_variables=[\"context\", \"topic\"]\\n)\\n\\nllm = OpenAI(temperature=0)\\n\\nchain = LLMChain(llm=llm, prompt=PROMPT)\\n\\n', metadata={'source': 'langchain/docs/modules/chains/index_examples/vector_db_text_generation.ipynb'})]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Define the custom prompt template\n",
    "prompt_template = \"\"\"Given the function name '{function_name}', write an English language explanation of the function.\n",
    "Function Definition:\n",
    "{function_source}\n",
    "\"\"\"\n",
    "\n",
    "# Define the input variables for the prompt template\n",
    "input_variables = [\"function_name\", \"function_source\"]\n",
    "\n",
    "# Create a PromptTemplate object with the custom prompt template and input variables\n",
    "custom_prompt = PromptTemplate(template=prompt_template, input_variables=input_variables)\n",
    "\n",
    "# Create an LLMChain object with the custom prompt template\n",
    "llm = OpenAI(temperature=0)\n",
    "llm_chain = LLMChain(prompt=custom_prompt, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"\"\"\n",
    "def sum(xs):\n",
    "    count = 0\n",
    "    for x in xs:\n",
    "        count += x\n",
    "    return count\n",
    "\"\"\"\n",
    "\n",
    "name = \"sum\"\n",
    "\n",
    "input = {\n",
    "    \"function_name\": name,\n",
    "    \"function_source\": src\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the function name 'sum', write an English language explanation of the function.\n",
      "Function Definition:\n",
      "\n",
      "def sum(xs):\n",
      "    count = 0\n",
      "    for x in xs:\n",
      "        count += x\n",
      "    return count\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'function_name': 'sum',\n",
       " 'function_source': '\\ndef sum(xs):\\n    count = 0\\n    for x in xs:\\n        count += x\\n    return count\\n',\n",
       " 'text': '\\nThe sum function takes a list of numbers (xs) as an argument and returns the sum of all the numbers in the list. It does this by looping through each number in the list and adding it to a running total, which is then returned as the result.'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
